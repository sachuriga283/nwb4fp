{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db9e081-b6be-494d-82fe-9b81ac42107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.postprocessing as post\n",
    "from nwb4fp.postprocess.Get_positions import load_positions,load_positions_h5\n",
    "from nwb4fp.postprocess.get_potential_merge import get_potential_merge\n",
    "from spikeinterface.preprocessing import (bandpass_filter,\n",
    "                                           common_reference)\n",
    "import spikeinterface.exporters as sex\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import probeinterface as pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620331fa-4d33-42cc-a9d5-ab929efc1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "vedio_search_directory = r\"S:/Sachuriga/Ephys_Vedio/CR_CA1\"\n",
    "path=r\"S:/Sachuriga\\Ephys_Recording/CR_CA1/65283/65283_2023-10-01_15-14-45_A_phy_k\"\n",
    "raw_path=r\"S:\\Sachuriga\\Ephys_Recording\\CR_CA1\\65283\\65283_2023-10-01_15-14-45_A\"\n",
    "temp_folder=r\"C:/temp_wave\"\n",
    "if path.endswith(\"phy_k_manual\"):\n",
    "    num2cal = int(41)\n",
    "elif path.endswith(\"phy_k\"):\n",
    "    num2cal = int(35)\n",
    "\n",
    "temp = path[0 - num2cal:]\n",
    "path1 = temp.split(\"/\")\n",
    "UD = path1[1].split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4517313-fa79-4b8e-b32e-1b9e7d8267e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(columns=['File', 'competability','dlc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c3bb05-5c39-474b-93f7-4b2ddd6bdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = se.read_phy(folder_path=path, load_all_cluster_properties=True,exclude_cluster_groups = [\"noise\", \"mua\"])\n",
    "global_job_kwargs = dict(n_jobs=12, chunk_size=10000, chunk_duration=\"1s\", total_memory=\"32G\")\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "temp_path = path.split(\"_phy\")\n",
    "raw_path = temp_path[0]\n",
    "stream_name = 'Record Node 101#OE_FPGA_Acquisition_Board-100.Rhythm Data'\n",
    "try:\n",
    "    recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n",
    "except AssertionError:\n",
    "    try:\n",
    "        stream_name = 'Record Node 102#OE_FPGA_Acquisition_Board-101.Rhythm Data'\n",
    "        recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n",
    "    except AssertionError:\n",
    "        stream_name = 'Record Node 101#Acquisition_Board-100.Rhythm Data'\n",
    "        recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "723746a5-956e-4a73-a969-711ca617dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSY-236-F - cambridgeneurotech - 64ch - 6shanks\n",
      "Checking the sorting properties\n"
     ]
    }
   ],
   "source": [
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-236-F'\n",
    "probe = pi.get_probe(manufacturer, probe_name)\n",
    "print(probe)\n",
    "# probe.wiring_to_device('cambridgeneurotech_mini-amp-64')\n",
    "# map channels to device indices\n",
    "mapping_to_device = [\n",
    "    # connector J2 TOP\n",
    "    41, 39, 38, 37, 35, 34, 33, 32, 29, 30, 28, 26, 25, 24, 22, 20,\n",
    "    46, 45, 44, 43, 42, 40, 36, 31, 27, 23, 21, 18, 19, 17, 16, 14,\n",
    "    # connector J1 BOTTOM\n",
    "    55, 53, 54, 52, 51, 50, 49, 48, 47, 15, 13, 12, 11, 9, 10, 8,\n",
    "    63, 62, 61, 60, 59, 58, 57, 56, 7, 6, 5, 4, 3, 2, 1, 0\n",
    "]\n",
    "\n",
    "probe.set_device_channel_indices(mapping_to_device)\n",
    "probe.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"shank_ids\", \"device_channel_indices\"]]\n",
    "probegroup = pi.ProbeGroup()\n",
    "probegroup.add_probe(probe)\n",
    "\n",
    "pi.write_prb(f\"{probe_name}.prb\", probegroup, group_mode=\"by_shank\")\n",
    "recording_prb = recording.set_probe(probe, group_mode=\"by_shank\")\n",
    "rec = bandpass_filter(recording_prb, freq_min=300, freq_max=6000)\n",
    "rec_save = common_reference(rec, reference='global', operator='median')\n",
    "sorting.set_property(key='group', values = sorting.get_property(\"channel_group\"))\n",
    "print(f\"Checking the sorting properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "630c1881-9a81-48d5-ae4c-2c2f210b6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract waveforms shared_memory multi buffer: 100%|███████████████████████████████| 1001/1001 [00:04<00:00, 234.77it/s]\n",
      "extract waveforms memmap multi buffer: 100%|██████████████████████████████████████| 1001/1001 [00:06<00:00, 158.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:/Sachuriga/Ephys_Vedio/CR_CA1\\*65283*A2023-10-01*800000_sk_filtered.h5\n",
      "[]\n",
      "S:\\Sachuriga\\Ephys_Recording\\CR_CA1\\65283\\65283_2023-10-01_15-14-45_A merge complete\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    wf = si.extract_waveforms(rec_save, sorting, folder=fr\"{temp_folder}\", overwrite=True, \n",
    "                              sparse=True, method=\"by_property\",by_property=\"group\",max_spikes_per_unit=1000)\n",
    "    try:\n",
    "        arr_with_new_col = load_positions_h5(path,vedio_search_directory,raw_path,UD)\n",
    "        new_row = pd.DataFrame({'File': [raw_path], 'competability': \"can be merged\",'dlc': \"file exits\"})\n",
    "    except IndexError:\n",
    "        new_row = pd.DataFrame({'File': [raw_path], 'competability': \"can be merged\",'dlc': \"file not found\"})\n",
    "    print(f\"{raw_path} merge complete\")\n",
    "except AssertionError:\n",
    "        try:\n",
    "            arr_with_new_col = load_positions_h5(path,vedio_search_directory,raw_path,UD)\n",
    "            new_row = pd.DataFrame({'File': [raw_path], 'competability': \"can not be merged\",'dlc': \"file exits\"})\n",
    "        except IndexError:\n",
    "            new_row = pd.DataFrame({'File': [raw_path], 'competability': \"can not be merged\",'dlc': \"file not found\"})\n",
    "        print(f\"{raw_path} no merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40cf0d5-db57-4acc-9f0c-6c262c03fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_positions_h5(path,vedio_search_directory,folder_path,UD):\n",
    "    import glob\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    ''' # Parameters:\n",
    "\n",
    "        path: This parameter is not used in the function and can be removed.\n",
    "        vedio_search_directory: This is the directory where the function will look for CSV files containing position data.\n",
    "        folder_path: This is the directory where the function will look for .npy files containing timestamps and states.\n",
    "        UD: This is a list or array-like object containing parameters used to construct the search pattern for the CSV files.\n",
    "        Usage:\n",
    "\n",
    "        Call the function with the appropriate parameters. For example:\n",
    "        The function will print the search pattern it uses to find the CSV files.\n",
    "        The function returns a numpy array containing the position data with timestamps inserted as the first column.\n",
    "        Output:\n",
    "\n",
    "        The function returns a numpy array where each row corresponds to a position sample. The first column of the array contains the timestamps, and the remaining columns contain the position data.\n",
    "        Notes:\n",
    "\n",
    "        The function assumes a specific directory structure and file naming convention. Make sure your files and directories match these expectations.\n",
    "        The function only uses the first unique CSV file and the first unique .npy files it finds. If there are multiple matching files, only the first one is used.\n",
    "        The function extracts specific columns from the CSV file. If your CSV file has a different structure, you may need to modify the column names in the code.\n",
    "        The function assumes that state 3 in the states.npy file corresponds to the desired timestamps. If your states represent something different, you may need to modify the code.'''\n",
    "    if path.endswith(\"phy_k_manual\"):\n",
    "        num2cal = int(41)\n",
    "    elif path.endswith(\"phy_k\"):\n",
    "        num2cal = int(35)\n",
    "\n",
    "    temp = path[0 - num2cal:]\n",
    "    path1 = temp.split(\"/\")\n",
    "    UD = path1[1].split(\"_\")\n",
    "\n",
    "    search_pattern = os.path.join(vedio_search_directory, f\"*{UD[0]}*{UD[3]}{UD[1]}*800000_sk_filtered.h5\")\n",
    "    print(search_pattern)\n",
    "    search_pattern1 = os.path.join(folder_path, '**/**/TTL/timestamps.npy')\n",
    "    search_pattern3 = os.path.join(folder_path, '**/**/TTL/states.npy')\n",
    "    #search_pattern2 = os.path.join(folder_path, '**/**/continuous/*/timestamps.npy')\n",
    "\n",
    "    # Use glob to find files matching the pattern\n",
    "\n",
    "    matching_files = glob.glob(search_pattern,recursive=True)\n",
    "    matching_files = np.unique(matching_files)\n",
    "    print(matching_files)\n",
    "    try:\n",
    "        dlc_path=Path(matching_files[0])\n",
    "        print(\"Used a 800000 iteration files\")\n",
    "    except IndexError:\n",
    "        try:\n",
    "            search_pattern = os.path.join(vedio_search_directory, f\"*{UD[0]}*{UD[3]}{UD[1]}*600000_sk_filtered.h5\")\n",
    "            matching_files = glob.glob(search_pattern,recursive=True)\n",
    "            matching_files = np.unique(matching_files)\n",
    "            mt_path=matching_files[0]\n",
    "            dlc_path=Path(mt_path)\n",
    "            print(\"Used a 600000 iteration files\")\n",
    "        except IndexError:\n",
    "            raise IndexError('No file found')\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_hdf(dlc_path, \"df_with_missing\")\n",
    "    except KeyError:\n",
    "        df = pd.read_hdf(dlc_path)   \n",
    "    bodyparts = df.columns.get_level_values(\"bodyparts\").unique().to_list()\n",
    "    scorer = df.columns.get_level_values(0)[0]\n",
    "    \n",
    "    coords = df[scorer, 'individual1'][[('snout', 'x'), ('snout', 'y'), ('neck', 'x'), ('neck', 'y'), ('back4', 'x'), ('back4', 'y')]]\n",
    "    positions = np.float32(coords.to_numpy())\n",
    "    \n",
    "    print(positions.shape)\n",
    "    matching_files = glob.glob(search_pattern1,recursive=True)\n",
    "    matching_files = np.unique(matching_files)\n",
    "    print(matching_files)\n",
    "    v_time = np.load(matching_files[0])\n",
    "    matching_files = glob.glob(search_pattern3,recursive=True)\n",
    "    matching_files = np.unique(matching_files)\n",
    "    v_state = np.load(matching_files[0])\n",
    "    f_time = v_time[np.where(v_state==3)[0]]\n",
    "    if f_time.shape[0] == 0:\n",
    "        try:\n",
    "            f_time = v_time[np.where(v_state==6)[0]]\n",
    "        except ValueError:\n",
    "            f_time = v_time[np.where(v_state==3)[0]]\n",
    "        print('Vedio is 25Hz')\n",
    "\n",
    "    arr_with_new_col =  np.insert(positions , 0, f_time[:len(positions)], axis=1) # type: ignore\n",
    "    return arr_with_new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76945e-818e-4962-8d49-1fa127065b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
